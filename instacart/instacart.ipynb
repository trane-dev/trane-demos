{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"instacart\"\n",
    "\n",
    "target_table = \"orders\"\n",
    "time_col = \"order_time\"\n",
    "\n",
    "window_size = \"2w\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_instacart_data\n",
    "\n",
    "dataframes, metadata = load_instacart_data(nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trane import ProblemGenerator\n",
    "\n",
    "problem_generator = ProblemGenerator(\n",
    "    metadata=metadata,\n",
    "    window_size=window_size,\n",
    "    target_table=target_table,\n",
    ")\n",
    "problems = problem_generator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 54 problems from 3 columns\n"
     ]
    }
   ],
   "source": [
    "num_columns = dataframes[target_table].shape[1]\n",
    "print(f\"generated {len(problems)} problems from {num_columns} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For each <order_id> predict if there exists a record in next 2w days\n",
      "For each <order_id> predict if there exists a record with <user_id> greater than <None> in next 2w days\n",
      "For each <order_id> predict if there exists a record with <user_id> less than <None> in next 2w days\n",
      "For each <order_id> predict the average <user_id> in all related records in next 2w days\n",
      "For each <order_id> predict the average <user_id> in all related records with <user_id> greater than <None> in next 2w days\n",
      "For each <order_id> predict the average <user_id> in all related records with <user_id> less than <None> in next 2w days\n",
      "For each <order_id> predict the maximum <user_id> in all related records in next 2w days\n",
      "For each <order_id> predict the maximum <user_id> in all related records with <user_id> greater than <None> in next 2w days\n",
      "For each <order_id> predict the maximum <user_id> in all related records with <user_id> less than <None> in next 2w days\n",
      "For each <order_id> predict the minimum <user_id> in all related records in next 2w days\n",
      "For each <order_id> predict the minimum <user_id> in all related records with <user_id> greater than <None> in next 2w days\n",
      "For each <order_id> predict the minimum <user_id> in all related records with <user_id> less than <None> in next 2w days\n",
      "For each <order_id> predict the number of records in next 2w days\n",
      "For each <order_id> predict the number of records with <user_id> greater than <None> in next 2w days\n",
      "For each <order_id> predict the number of records with <user_id> less than <None> in next 2w days\n",
      "For each <order_id> predict the total <user_id> in all related records in next 2w days\n",
      "For each <order_id> predict the total <user_id> in all related records with <user_id> greater than <None> in next 2w days\n",
      "For each <order_id> predict the total <user_id> in all related records with <user_id> less than <None> in next 2w days\n",
      "For each <user_id> predict if there exists a record in next 2w days\n",
      "For each <user_id> predict if there exists a record with <user_id> greater than <None> in next 2w days\n",
      "For each <user_id> predict if there exists a record with <user_id> less than <None> in next 2w days\n",
      "For each <user_id> predict the average <user_id> in all related records in next 2w days\n",
      "For each <user_id> predict the average <user_id> in all related records with <user_id> greater than <None> in next 2w days\n",
      "For each <user_id> predict the average <user_id> in all related records with <user_id> less than <None> in next 2w days\n",
      "For each <user_id> predict the maximum <user_id> in all related records in next 2w days\n",
      "For each <user_id> predict the maximum <user_id> in all related records with <user_id> greater than <None> in next 2w days\n",
      "For each <user_id> predict the maximum <user_id> in all related records with <user_id> less than <None> in next 2w days\n",
      "For each <user_id> predict the minimum <user_id> in all related records in next 2w days\n",
      "For each <user_id> predict the minimum <user_id> in all related records with <user_id> greater than <None> in next 2w days\n",
      "For each <user_id> predict the minimum <user_id> in all related records with <user_id> less than <None> in next 2w days\n",
      "For each <user_id> predict the number of records in next 2w days\n",
      "For each <user_id> predict the number of records with <user_id> greater than <None> in next 2w days\n",
      "For each <user_id> predict the number of records with <user_id> less than <None> in next 2w days\n",
      "For each <user_id> predict the total <user_id> in all related records in next 2w days\n",
      "For each <user_id> predict the total <user_id> in all related records with <user_id> greater than <None> in next 2w days\n",
      "For each <user_id> predict the total <user_id> in all related records with <user_id> less than <None> in next 2w days\n",
      "Predict if there exists a record in next 2w days\n",
      "Predict if there exists a record with <user_id> greater than <None> in next 2w days\n",
      "Predict if there exists a record with <user_id> less than <None> in next 2w days\n",
      "Predict the average <user_id> in all related records in next 2w days\n",
      "Predict the average <user_id> in all related records with <user_id> greater than <None> in next 2w days\n",
      "Predict the average <user_id> in all related records with <user_id> less than <None> in next 2w days\n",
      "Predict the maximum <user_id> in all related records in next 2w days\n",
      "Predict the maximum <user_id> in all related records with <user_id> greater than <None> in next 2w days\n",
      "Predict the maximum <user_id> in all related records with <user_id> less than <None> in next 2w days\n",
      "Predict the minimum <user_id> in all related records in next 2w days\n",
      "Predict the minimum <user_id> in all related records with <user_id> greater than <None> in next 2w days\n",
      "Predict the minimum <user_id> in all related records with <user_id> less than <None> in next 2w days\n",
      "Predict the number of records in next 2w days\n",
      "Predict the number of records with <user_id> greater than <None> in next 2w days\n",
      "Predict the number of records with <user_id> less than <None> in next 2w days\n",
      "Predict the total <user_id> in all related records in next 2w days\n",
      "Predict the total <user_id> in all related records with <user_id> greater than <None> in next 2w days\n",
      "Predict the total <user_id> in all related records with <user_id> less than <None> in next 2w days\n"
     ]
    }
   ],
   "source": [
    "for problem in problems:\n",
    "    print(problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **For each `<order_id>` predict the number of records in next 2w days**: Knowing the predicted number of records related to an order can help in various aspects like inventory management, customer service, and other operational efficiencies.\n",
    "\n",
    "2. **For each `<user_id>` predict the number of records in next 2w days**: Similar to the above, but focusing on user behavior rather than orders. This could be crucial for understanding customer engagement and could feed into personalized marketing efforts.\n",
    "\n",
    "3. **Predict the number of records in next 2w days**: This can provide a high-level view of expected system load, required manpower, and other resources. Good for strategic planning.\n",
    "\n",
    "4. **For each `<order_id>` predict if there exists a record in next 2w days**: This could be useful for triggering other processes like follow-up customer communications, review solicitations, or even automated systems like re-order reminders.\n",
    "\n",
    "5. **For each `<user_id>` predict if there exists a record in next 2w days**: Understanding which users are likely to be active in the short term can be useful for targeted marketing campaigns, customer retention efforts, or fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"generated_problems.txt\", \"w\") as text_file:\n",
    "    for idx, p in enumerate(problems):\n",
    "        print(idx, p, file=text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find a relevant problem\n",
    "Look through the generated_problems.txt file and find the ID of an interesting problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = problems[273].execute(df, num_examples_per_instance=-1)\n",
    "problems[273]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "Using Featuretools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import featuretools as ft\n",
    "\n",
    "es = ft.EntitySet(name)\n",
    "\n",
    "es.add_dataframe(\n",
    "    dataframe=df.reset_index(),\n",
    "    dataframe_name=\"order_products\",\n",
    "    time_index=\"orders.order_date\",\n",
    "    index=\"__id__\",\n",
    ")\n",
    "\n",
    "es.normalize_dataframe(\n",
    "    base_dataframe_name=\"order_products\",\n",
    "    new_dataframe_name=\"products\",\n",
    "    index=\"product_id\",\n",
    "    additional_columns=[\"products.aisle_id\", \"products.aisles.aisle\", \"products.department_id\", \"products.departments.department\", \"products.product_name\"],\n",
    "    make_time_index=False,\n",
    ")\n",
    "\n",
    "es.normalize_dataframe(\n",
    "    base_dataframe_name=\"order_products\",\n",
    "    new_dataframe_name=\"orders\",\n",
    "    index=\"order_id\",\n",
    "    additional_columns=[\"orders.user_id\", \"orders.eval_set\", \"orders.order_number\", \"orders.order_dow\", \"orders.order_hour_of_day\", \"orders.days_since_prior_order\"],\n",
    "    make_time_index=False,\n",
    ")\n",
    "\n",
    "es.normalize_dataframe(\n",
    "    base_dataframe_name=\"products\",\n",
    "    new_dataframe_name=\"aisles\",\n",
    "    index=\"products.aisle_id\",\n",
    "    make_time_index=False,\n",
    ")\n",
    "\n",
    "es.normalize_dataframe(\n",
    "    base_dataframe_name=\"products\",\n",
    "    new_dataframe_name=\"departments\",\n",
    "    index=\"products.department_id\",\n",
    "    make_time_index=False,\n",
    ")\n",
    "\n",
    "es.normalize_dataframe(\n",
    "    base_dataframe_name=\"orders\",\n",
    "    new_dataframe_name=\"orders.user_id\",\n",
    "    index=\"orders.user_id\",\n",
    "    make_time_index=False,\n",
    ")\n",
    "\n",
    "es.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm, fd = ft.dfs(\n",
    "    entityset=es,\n",
    "    target_dataframe_name=entity_col,\n",
    "    cutoff_time=ex,\n",
    "    cutoff_time_in_index=True,\n",
    "    include_cutoff_time=False,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "fm.reset_index(drop=True, inplace=True)\n",
    "y = fm.ww.pop('_execute_operations_on_df')\n",
    "X = fm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train/Test split by time cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cutoff = pd.to_datetime(\"2023-09-01\")\n",
    "X_train, y_train = X[ex[\"time\"] <= train_cutoff], y[ex[\"time\"] <= train_cutoff]\n",
    "X_test, y_test = X[ex[\"time\"] > train_cutoff], y[ex[\"time\"] > train_cutoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "model = lgb.LGBMClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run inference on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    ax = sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_confusion_matrix(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "print(f\"AUC: {round(roc_auc_score(y_test, y_hat), 2)}\")\n",
    "print(f\"F1 Score: {round(f1_score(y_test, y_hat), 2)}\")\n",
    "print(f\"Accuracy: {round(accuracy_score(y_test, y_hat), 2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
